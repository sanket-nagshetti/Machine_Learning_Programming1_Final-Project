{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5tr_jEBnh-jv"
   },
   "source": [
    "# Title:\n",
    "\n",
    "## Group Member Names :\n",
    "\n",
    "#### Sanket Nagshetti (200565218)\n",
    "#### Shayam Kishore Kotapati (200600387)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PeKSxMvrh-j0"
   },
   "source": [
    "### INTRODUCTION:\n",
    "*********************************************************************************************************************\n",
    "#### AIM :\n",
    "#### This project focuses on enhancing the accuracy of supervised machine learning algorithms for simplified cancer \n",
    "#### classification. By employing various models and fine-tuning parameters, we aim to optimize the system's \n",
    "#### performance and achieve more precise  diagnostic results.\n",
    "#### Github Repo:\n",
    "https://github.com/sanket-nagshetti/Machine_Learning_Programming1_Final-Project\n",
    "*********************************************************************************************************************\n",
    "#### DESCRIPTION OF PAPER:\n",
    "\n",
    "This project developed a decision support system (DSS) to assist clinicians in early cancer diagnosis by classifying five different cancer types using 20 cancer exomes and their mutations. Initially, several supervised machine learning algorithms—K-nearest neighbor (KNN), support vector machine (SVM), decision tree, naïve bayes, and random forest—were tested, with decision tree and random forest showing the highest preliminary accuracy. To improve accuracy, 16 key features were selected, and datasets were balanced using SMOTE. The refined Random Forest model achieved an accuracy of 82% and demonstrated superior performance with an area under the curve closer to 1 compared to the decision tree model. The model was validated using Matthew's correlation coefficient (MCC), with high scores confirming its reliability. The final model was deployed as a user-friendly web application using Streamlit, facilitating effective cancer classification.\n",
    "\n",
    "*********************************************************************************************************************\n",
    "#### PROBLEM STATEMENT :\n",
    "\n",
    "#### This project seeks to improve the accuracy of cancer classification using supervised machine learning algorithms by optimizing model performance and fine-tuning parameters for more precise diagnostics.\n",
    "\n",
    "*********************************************************************************************************************\n",
    "#### CONTEXT OF THE PROBLEM:\n",
    "\n",
    " The project focuses on enhancing the accuracy of cancer classification by utilizing supervised machine learning  algorithms. By optimizing model performance and fine-tuning parameters, the goal is to achieve more precise     diagnostics. This involves experimenting with various machine learning models, adjusting their settings, and  evaluating their effectiveness on cancer datasets to improve overall classification accuracy.\n",
    "\n",
    "*********************************************************************************************************************\n",
    "#### SOLUTION:\n",
    "\n",
    "To potentially increase accuracy, try using the MLPClassifier to see if it improves performance. Additionally, increase the number of trees in the Random Forest from 10 to 100. Also, optimize hyperparameters by limiting the maximum tree depth to 10 and adjusting the minimum samples required for splits and leaves.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "77PIPLQ-h-j1"
   },
   "source": [
    "# Background\n",
    "*********************************************************************************************************************\n",
    "\n",
    "\n",
    "|Reference|Explanation|Dataset/Input|Weakness|\n",
    "|Pagel KA, Kim R, Moad K, et al.|Integrated informatics analysis of cancerrelated variants|Cancer Dataset| Criteria to choose parameters not clearly mentioned|\n",
    "|------|------|------|------|\n",
    "\n",
    "\n",
    "\n",
    "*********************************************************************************************************************\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "deODH3tMh-j2"
   },
   "source": [
    "# Implement paper code :\n",
    "*********************************************************************************************************************\n",
    "#### Adding new classification technique MLPClassifier to check whether performance will increase or not\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Initialize the MLP model\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100,), max_iter=500, random_state=42)\n",
    "# Fit the MLP model on the balanced training data\n",
    "mlp.fit(X_train_ros, y_train_ros)\n",
    "# Predict using the MLP model on the test data\n",
    "y_pred_mlp = mlp.predict(x_test)\n",
    "# Print the classification report for MLP model\n",
    "print(\"MLP Classifier Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_mlp))\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Initialize models\n",
    "dtc = DecisionTreeClassifier()\n",
    "rf = RandomForestClassifier()\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100,), max_iter=500, random_state=42)\n",
    "\n",
    "# Fit models on the balanced training data\n",
    "dtc.fit(X_train_ros, y_train_ros)\n",
    "rf.fit(X_train_ros, y_train_ros)\n",
    "mlp.fit(X_train_ros, y_train_ros)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_dtc = dtc.predict(x_test)\n",
    "y_pred_rf = rf.predict(x_test)\n",
    "y_pred_mlp = mlp.predict(x_test)\n",
    "\n",
    "# Print classification reports\n",
    "print(\"Decision Tree Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_dtc))\n",
    "\n",
    "print(\"Random Forest Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "\n",
    "print(\"MLP Classifier Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_mlp))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Tunning parameters to increase the performance of RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Initialize and fit SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_ros, y_train_ros = smote.fit_resample(x_train, y_train)\n",
    "\n",
    "# Initialize Random Forest Classifier with optimized parameters\n",
    "classifier1 = RandomForestClassifier(\n",
    "    n_estimators=100,         # A larger number of trees for better performance\n",
    "    max_depth=10,             # Limit depth to avoid overfitting\n",
    "    min_samples_split=5,      # Minimum samples required to split an internal node\n",
    "    min_samples_leaf=2,       # Minimum samples required to be at a leaf node\n",
    "    max_features='sqrt',      # Use a subset of features for each split\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit the classifier\n",
    "classifier1.fit(X_train_ros, y_train_ros)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred1 = classifier1.predict(x_test)\n",
    "\n",
    "# Print classification report\n",
    "print(\"Random Forest Classification Report:\")\n",
    "print(classification_report(y_test, y_pred1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2gkHhku9h-j2"
   },
   "source": [
    "*********************************************************************************************************************\n",
    "### Contribution  Code :\n",
    "*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-YdFCgWoh-j3"
   },
   "source": [
    "### Results :\n",
    "*******************************************************************************************************************************\n",
    "Accuracy for MLP Classifier Classification Report = 75%\n",
    "\n",
    "Accuracy for Random forest Classifier after tunning parameter = 82%\n",
    "\n",
    "\n",
    "#### Observations :\n",
    "*******************************************************************************************************************************\n",
    "*\n",
    "Introducing the MLP model did not result in a significant improvement over the Random Forest model, which continues to be the best-performing model in terms of overall accuracy and F1-scores. The MLP model provides similar performance to the Decision Tree model but does not surpass Random Forest. Therefore, while MLP contributes to the analysis, Random Forest remains the preferred model for this classification problem.\n",
    "\n",
    "*\n",
    "We improved the accuracy from 81% to 82% by increasing the number of trees in the Random Forest from 10 to 100, which enhanced the model's robustness. We optimized hyperparameters, such as limiting the maximum tree depth to 10 and adjusting the minimum samples for splits and leaves, which helped balance model complexity and generalization. Additionally, we implemented feature selection to focus on the most important features, reducing noise and improving performance. These changes collectively refined the model, leading to the increased accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s3JVj9dKh-j3"
   },
   "source": [
    "### Conclusion and Future Direction :\n",
    "*******************************************************************************************************************************\n",
    "\n",
    "In conclusion, the Random Forest model remains the superior choice for our classification problem, outperforming both the Decision Tree and MLP models in terms of accuracy and F1-scores. While the introduction of the MLP model did not yield a significant improvement, our efforts to enhance the Random Forest model proved successful. By increasing the number of trees, optimizing hyperparameters, and implementing feature selection, we managed to improve the model's accuracy from 81% to 82%. These refinements have made the Random Forest model more robust and better suited to the classification task at hand, solidifying its position as the preferred model.\n",
    "\n",
    "#### Learnings :\n",
    "\n",
    "*******************************************************************************************************************************\n",
    "This experience highlighted the importance of model selection, with Random Forest outperforming MLP and Decision Tree models in accuracy and F1-scores. Incremental improvements, such as increasing tree count and optimizing hyperparameters, significantly enhanced the Random Forest model's performance, improving accuracy from 81% to 82%. Feature selection further refined the model by focusing on key features, reducing noise. Although the MLP model didn't surpass Random Forest, exploring alternative models provided valuable insights. Overall, this process emphasized the value of careful tuning and robustness in developing effective machine learning models.\n",
    "\n",
    "\n",
    "#### Results Discussion :\n",
    "The results of our analysis demonstrate that the Random Forest model consistently outperformed both the Decision Tree and MLP models in terms of accuracy and F1-scores. Despite the introduction of the MLP model, which offered performance comparable to the Decision Tree model, it did not surpass the accuracy and robustness of Random Forest. This suggests that for this particular classification problem, Random Forest is better suited due to its ability to handle complex patterns and interactions within the data.\n",
    "\n",
    "The improvement in Random Forest's accuracy from 81% to 82% was achieved through targeted enhancements, including increasing the number of trees and fine-tuning hyperparameters like maximum tree depth and the minimum samples required for splits and leaves. These adjustments helped balance the model’s complexity and generalization ability, resulting in a more robust and accurate model.\n",
    "\n",
    "Moreover, the implementation of feature selection played a crucial role in this improvement by reducing noise and focusing on the most important features, which streamlined the model's decision-making process. This not only improved accuracy but also contributed to the model’s overall efficiency.\n",
    "\n",
    "In summary, while exploring alternative models like MLP is valuable, the Random Forest model remains the most effective choice for this classification task. The results underscore the importance of model selection, hyperparameter tuning, and feature selection in optimizing machine learning models for better performance.\n",
    "*******************************************************************************************************************************\n",
    "#### Limitations :\n",
    "This analysis has several limitations. First, only a few models were explored, potentially overlooking more effective alternatives. The results are specific to the dataset used, limiting generalizability. Increasing the number of trees in the Random Forest model improved accuracy but at a higher computational cost, which may not be feasible in resource-limited settings. Additionally, there is a risk of overfitting, and the model's performance is sensitive to hyperparameter settings and dependent on the quality of selected features. These factors could affect the model's robustness and replication in different scenarios.\n",
    "\n",
    "\n",
    "*******************************************************************************************************************************\n",
    "#### Future Extension :\n",
    "\n",
    "Future extensions could include exploring additional models like Gradient Boosting or Support Vector Machines to identify more effective alternatives. Testing on different datasets would help validate the Random Forest model’s generalizability. Further optimization through advanced techniques like automated hyperparameter tuning or ensemble methods could improve accuracy while managing computational costs. Addressing overfitting with cross-validation or regularization would enhance robustness. Additionally, refining feature engineering and selection could further improve model performance, providing a deeper understanding of its capabilities across various contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ATXtFdtBh-j4"
   },
   "source": [
    "# References:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CQnMSAf-h-j4"
   },
   "source": [
    "1. Liu Sheng OR. Decision support for healthcare in a new information age. Decis\n",
    "Support Syst. 2000;30:101-103.\n",
    "2. Hicks JK, Dunnenberger HM, Gumpper KF, Haidar CE, Hoffman JM.\n",
    "Integrating pharmacogenomics into electronic health records with clinical\n",
    "decision support. Am J Health Syst Pharm. 2016;73:1967-1976."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
